{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def load_query(query_load_path):\n",
    "\tfile = open(query_load_path)\n",
    "\tnodes_list = []\n",
    "\tedges_list = []\n",
    "\tlabel_cnt = 0\n",
    "\n",
    "\tfor line in file:\n",
    "\t\tif line.strip().startswith(\"v\"):\n",
    "\t\t\ttokens = line.strip().split()\n",
    "\t\t\t# v nodeID labelID\n",
    "\t\t\tid = int(tokens[1])\n",
    "\t\t\ttmp_labels = [int(tokens[2])] # (only one label in the query node)\n",
    "\t\t\t#tmp_labels = [int(token) for token in tokens[2 : ]]\n",
    "\t\t\tlabels = [] if -1 in tmp_labels else tmp_labels\n",
    "\t\t\tlabel_cnt += len(labels)\n",
    "\t\t\tnodes_list.append((id, {\"labels\": labels}))\n",
    "\n",
    "\t\tif line.strip().startswith(\"e\"):\n",
    "\t\t\t# e srcID dstID labelID1 labelID2....\n",
    "\t\t\ttokens = line.strip().split()\n",
    "\t\t\tsrc, dst = int(tokens[1]), int(tokens[2])\n",
    "\t\t\ttmp_labels = [int(tokens[3])]\n",
    "\t\t\t#tmp_labels = [int(token) for token in tokens[3 : ]]\n",
    "\t\t\tlabels = [] if -1 in tmp_labels else tmp_labels\n",
    "\t\t\tedges_list.append((src, dst, {\"labels\": labels}))\n",
    "\n",
    "\tquery = nx.Graph()\n",
    "\tquery.add_nodes_from(nodes_list)\n",
    "\tquery.add_edges_from(edges_list)\n",
    "\tfile.close()\n",
    "\treturn query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from graph_operation import match_bfs\n",
    "import subprocess\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import os\n",
    "\n",
    "encoding = 'utf-8'\n",
    "\n",
    "def load_g_graph(g_file):\n",
    "    nid = list()\n",
    "    nlabel = list()\n",
    "    nindeg = list()\n",
    "    elabel = list()\n",
    "    e_u = list()\n",
    "    e_v = list()\n",
    "    with open(g_file) as f2:\n",
    "        num_nodes = int(f2.readline().rstrip())\n",
    "        v_neigh = list()\n",
    "        for i in range(num_nodes):\n",
    "            temp_list = list()\n",
    "            v_neigh.append(temp_list)\n",
    "        for i in range(num_nodes):\n",
    "            node_info = f2.readline()\n",
    "            node_id, node_label = node_info.rstrip().split()\n",
    "            nid.append(int(node_id))\n",
    "            nlabel.append(int(node_label))\n",
    "        while True:\n",
    "            line = f2.readline()\n",
    "            # read until the end of the file.\n",
    "            if not line:\n",
    "                break\n",
    "            temp_indeg = int(line.strip())\n",
    "            nindeg.append(temp_indeg)\n",
    "            if temp_indeg == 0:\n",
    "                continue\n",
    "            for i in range(temp_indeg):\n",
    "                edge_info = f2.readline().rstrip().split()\n",
    "                if len(edge_info) == 2:\n",
    "                    edge_label = 1\n",
    "                else:\n",
    "                    edge_label = int(edge_info[-1])\n",
    "                e_u.append(int(edge_info[0]))\n",
    "                e_v.append(int(edge_info[1]))\n",
    "                v_neigh[int(edge_info[0])].append(int(edge_info[1]))\n",
    "                # v_neigh[int(edge_info[1])].append(int(edge_info[0]))\n",
    "                elabel.append(edge_label)\n",
    "    g_nid = deepcopy(nid)\n",
    "    g_nlabel = deepcopy(nlabel)\n",
    "    g_indeg = deepcopy(nindeg)\n",
    "    g_edges = [deepcopy(e_u), deepcopy(e_v)]\n",
    "    g_elabel = deepcopy(elabel)\n",
    "    g_v_neigh = deepcopy(v_neigh)\n",
    "    g_label_dict = defaultdict(list)\n",
    "    for i in range(len(g_nlabel)):\n",
    "        g_label_dict[g_nlabel[i]].append(i)\n",
    "    graph_info = [\n",
    "        g_nid,\n",
    "        g_nlabel,\n",
    "        g_indeg,\n",
    "        g_edges,\n",
    "        g_elabel,\n",
    "        g_v_neigh,\n",
    "        g_label_dict\n",
    "    ]\n",
    "    return graph_info\n",
    "\n",
    "\n",
    "def load_p_data(p_file):\n",
    "    nid = list()\n",
    "    nlabel = list()\n",
    "    nindeg = list()\n",
    "    elabel = list()\n",
    "    e_u = list()\n",
    "    e_v = list()\n",
    "\n",
    "    with open(p_file) as f1:\n",
    "        num_nodes = int(f1.readline().rstrip())\n",
    "        v_neigh = list()\n",
    "        for i in range(num_nodes):\n",
    "            temp_list = list()\n",
    "            v_neigh.append(temp_list)\n",
    "        for i in range(num_nodes):\n",
    "            node_info = f1.readline()\n",
    "            node_id, node_label = node_info.rstrip().split()\n",
    "            # print(node_id)\n",
    "            # print(node_label)\n",
    "            nid.append(int(node_id))\n",
    "            nlabel.append(int(node_label))\n",
    "        while True:\n",
    "            line = f1.readline()\n",
    "            # read until the end of the file.\n",
    "            if not line:\n",
    "                break\n",
    "            temp_indeg = int(line.strip())\n",
    "            nindeg.append(temp_indeg)\n",
    "            if temp_indeg == 0:\n",
    "                continue\n",
    "            for i in range(temp_indeg):\n",
    "                edge_info = f1.readline().rstrip().split()\n",
    "                if len(edge_info) == 2:\n",
    "                    edge_label = 1\n",
    "                else:\n",
    "                    edge_label = int(edge_info[-1])\n",
    "                e_u.append(int(edge_info[0]))\n",
    "                e_v.append(int(edge_info[1]))\n",
    "                v_neigh[int(edge_info[0])].append(int(edge_info[1]))\n",
    "                # v_neigh[int(edge_info[1])].append(int(edge_info[0]))\n",
    "                elabel.append(edge_label)\n",
    "    p_nid = deepcopy(nid)\n",
    "    p_nlabel = deepcopy(nlabel)\n",
    "    p_indeg = deepcopy(nindeg)\n",
    "    p_edges = [deepcopy(e_u), deepcopy(e_v)]\n",
    "    p_elabel = deepcopy(elabel)\n",
    "    p_v_neigh = [deepcopy(v_list) for v_list in v_neigh]\n",
    "    p_label_dict = defaultdict(list)\n",
    "    for i in range(len(p_nlabel)):\n",
    "        p_label_dict[p_nlabel[i]].append(i)\n",
    "    pattern_info = [\n",
    "        p_nid,\n",
    "        p_nlabel,\n",
    "        p_indeg,\n",
    "        p_edges,\n",
    "        p_elabel,\n",
    "        p_v_neigh,\n",
    "        p_label_dict\n",
    "    ]\n",
    "    return pattern_info\n",
    "\n",
    "def load_graph(file_path):\n",
    "    # data information contains:\n",
    "    # return [0: id 1: label 2: degree 3: edge_info 4: edge_label 5: vertex neighbor 6: label_dict]\n",
    "    nid = list()\n",
    "    nlabel = list()\n",
    "    nindeg = list()\n",
    "    elabel = list()\n",
    "    e_u = list()\n",
    "    e_v = list()\n",
    "    v_neigh = list()\n",
    "    \n",
    "    with open(file_path) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            if line.startswith('v'):\n",
    "                tokens = line.split()\n",
    "                id = int(tokens[1])\n",
    "                label = int(tokens[2])\n",
    "                degree = int(tokens[3])\n",
    "                nid.append(id)\n",
    "                nlabel.append(label)\n",
    "                nindeg.append(degree)\n",
    "                v_neigh.append(list())\n",
    "            if line.startswith('e'):\n",
    "                tokens = line.split()\n",
    "                src = int(tokens[1])\n",
    "                dst = int(tokens[2])\n",
    "                label = 0\n",
    "                if len(tokens) > 3:\n",
    "                    label = int(tokens[3])\n",
    "                elabel.append(label)\n",
    "                e_u.append(src)\n",
    "                e_v.append(dst)\n",
    "                v_neigh[src].append(dst)\n",
    "                v_neigh[dst].append(src)\n",
    "    p_nid = deepcopy(nid)\n",
    "    p_nlabel = deepcopy(nlabel)\n",
    "    p_indeg = deepcopy(nindeg)\n",
    "    p_edges = [deepcopy(e_u), deepcopy(e_v)]\n",
    "    p_elabel = deepcopy(elabel)\n",
    "    p_v_neigh = [deepcopy(v_list) for v_list in v_neigh]\n",
    "    p_label_dict = defaultdict(list)\n",
    "    for i in range(len(p_nlabel)):\n",
    "        p_label_dict[p_nlabel[i]].append(i)\n",
    "    graph_info = [\n",
    "        p_nid,\n",
    "        p_nlabel,\n",
    "        p_indeg,\n",
    "        p_edges,\n",
    "        p_elabel,\n",
    "        p_v_neigh,\n",
    "        p_label_dict\n",
    "    ]\n",
    "    return graph_info\n",
    "\n",
    "\n",
    "def graph_depth(graph, starting_vertex):\n",
    "    visited = list()\n",
    "    queue = list()\n",
    "    visit_count = 0\n",
    "    depth = 0\n",
    "    nodes = graph[0]\n",
    "    num_nodes = len(nodes)\n",
    "    neigh_info = graph[5]\n",
    "    queue.append(starting_vertex)\n",
    "    visited.append(starting_vertex)\n",
    "    visit_count += 1\n",
    "    while visit_count<num_nodes:\n",
    "        current_node = queue.pop(0)\n",
    "        current_neighbors = neigh_info[current_node]\n",
    "        for u in current_neighbors:\n",
    "            if u not in visited:\n",
    "                queue.append(u)\n",
    "                visited.append(u)\n",
    "                visit_count += 1\n",
    "        depth+=1\n",
    "        print('depth, visited: ', depth, visited)\n",
    "    return depth\n",
    "\n",
    "class Filtering:\n",
    "    def __init__(self, pattern, data_graph):\n",
    "        # data information contains:\n",
    "        # 0: id 1: label 2: degree 3: edge_info 4: edge_label 5: vertex neighbor 6: label_dict\n",
    "        self.pattern = pattern\n",
    "        self.data_graph = data_graph\n",
    "\n",
    "    def GQL_filter(self):\n",
    "        # generate the candidate set using GraphQL.\n",
    "\n",
    "        # get candidates by NLF, as initialization.\n",
    "        local_candidates, candidate_count = self.generate_general_candidates()\n",
    "        invalid_vertex_id = -1\n",
    "\n",
    "        # basic information\n",
    "        query_vertex_num = len(self.pattern[0])\n",
    "        data_vertex_num = len(self.data_graph[0])\n",
    "        query_max_degree = max(self.pattern[2])\n",
    "        data_max_degree = max(self.data_graph[2])\n",
    "\n",
    "        # generate valid candidate list\n",
    "        valid_candidate = list()\n",
    "        for i in range(query_vertex_num):\n",
    "            temp_list = [False] * data_vertex_num\n",
    "            for v in local_candidates[i]:\n",
    "                temp_list[v] = True\n",
    "            valid_candidate.append(deepcopy(temp_list))\n",
    "\n",
    "        # global refinement\n",
    "        for l in range(2):\n",
    "            for i in range(query_vertex_num):\n",
    "                query_vertex = i\n",
    "                for j in range(candidate_count[i]):\n",
    "                    data_vertex = local_candidates[i][j]\n",
    "                    if data_vertex == invalid_vertex_id:\n",
    "                        continue\n",
    "                    if not self.verify_exact_twig_iso(query_vertex, data_vertex, query_vertex_num, data_vertex_num,\n",
    "                                                      query_max_degree, data_max_degree, valid_candidate):\n",
    "                        local_candidates[query_vertex][j] = invalid_vertex_id\n",
    "                        valid_candidate[query_vertex][data_vertex] = False\n",
    "\n",
    "        candidates, candidate_count = self.compact_candidate(local_candidates, query_vertex_num)\n",
    "\n",
    "        return candidates, candidate_count\n",
    "\n",
    "    def generate_general_candidates(self):\n",
    "        # generate the candidate using NLF.\n",
    "        p_label = self.pattern[1]\n",
    "        p_degree = self.pattern[2]\n",
    "        g_degree = self.data_graph[2]\n",
    "        candidates = list()\n",
    "        candidate_count = [0] * len(p_label)\n",
    "        for i in range(len(p_label)):    # num of candidate vertices.\n",
    "            selected_label_vertices = self.get_vertices_by_label(p_label[i])   # select nodes with the same label\n",
    "            temp_list = []\n",
    "            for v in selected_label_vertices:\n",
    "                if g_degree[v] >= p_degree[i]:                                 # check the degree.\n",
    "                    # check NLF\n",
    "                    if self.check_NLF(i, v):\n",
    "                        temp_list.append(v)\n",
    "                        candidate_count[i] += 1\n",
    "            candidates.append(deepcopy(temp_list))\n",
    "\n",
    "        return candidates, candidate_count\n",
    "\n",
    "    def get_vertices_by_label(self, label):\n",
    "        selected_vertices = self.data_graph[6][label]\n",
    "        return selected_vertices\n",
    "\n",
    "    def verify_exact_twig_iso(self, query_vertex, data_vertex, query_vertex_num, data_vertex_num,\n",
    "                                                      query_max_degree, data_max_degree, valid_candidates):\n",
    "        # construct the bipartite graph and determine whether it is valid.\n",
    "        q_neighbors = self.pattern[5][query_vertex]\n",
    "        # print(q_neighbors)\n",
    "        d_neighbors = self.data_graph[5][data_vertex]\n",
    "        # print(d_neighbors)\n",
    "        left_partition_size = len(q_neighbors)\n",
    "        right_partition_size = len(d_neighbors)\n",
    "\n",
    "        # note that the initial might be wrong.\n",
    "        left_to_right_offset = [0] * (query_max_degree+1)  # query_max_degree + 1\n",
    "        left_to_right_edges = [None] * (query_max_degree * data_max_degree)   # query_max_degree * data_max_degree\n",
    "        left_to_right_match = [None] * query_max_degree   # query_max_degree\n",
    "\n",
    "        # right_to_left_match = list()   # data_max_degree\n",
    "        # match_visited = list()         # data_max_degree + 1\n",
    "        # match_queue = list()           # query_vertex_num\n",
    "        # match_previous = list()        # data_max_degree + 1\n",
    "\n",
    "        # print(query_max_degree)\n",
    "        # print(data_max_degree)\n",
    "        # print(left_partition_size)\n",
    "        # print(right_partition_size)\n",
    "        # build the bipartite graph\n",
    "        edge_count = 0\n",
    "        for i in range(left_partition_size):\n",
    "            query_vertex_neighbor = q_neighbors[i]\n",
    "            left_to_right_offset[i] = edge_count\n",
    "            for j in range(right_partition_size):\n",
    "                data_vertex_neighbor = d_neighbors[j]\n",
    "                if valid_candidates[query_vertex_neighbor][data_vertex_neighbor]:\n",
    "                    edge_count+=1\n",
    "                    # print(edge_count)\n",
    "                    left_to_right_edges[edge_count] = j\n",
    "        left_to_right_offset[left_partition_size] = edge_count\n",
    "\n",
    "        # check if it is a semi-perfect match, process the left_to_right_match, find the ones that are not matched.\n",
    "        for i in range(left_partition_size):\n",
    "            if left_to_right_match[i] is None and left_to_right_offset[i] != left_to_right_offset[i+1]:\n",
    "                for j in range(left_to_right_offset[i], left_to_right_offset[i+1]):\n",
    "                    if left_to_right_edges[j] is not None:\n",
    "                        left_to_right_match[i] = left_to_right_edges[j]\n",
    "                        break\n",
    "        for i in range(left_partition_size):\n",
    "            if left_to_right_match[i] is None:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    # def is_valid_candidate(self, ):\n",
    "    def compact_candidate(self, local_candidates, query_vertex_num):\n",
    "        new_candidates = list()\n",
    "        new_candidate_count = [0] * query_vertex_num\n",
    "        for i in range(query_vertex_num):\n",
    "            query_vertx = i\n",
    "            temp_list = []\n",
    "            for j in range(len(local_candidates[query_vertx])):\n",
    "                if local_candidates[query_vertx][j] != -1:\n",
    "                    temp_list.append(local_candidates[query_vertx][j])\n",
    "                    new_candidate_count[query_vertx] += 1\n",
    "            new_candidates.append(deepcopy(temp_list))\n",
    "\n",
    "        return new_candidates, new_candidate_count\n",
    "\n",
    "    def check_NLF(self, query_vertex, data_vertex):\n",
    "        query_neighbors = self.pattern[5][query_vertex]\n",
    "        data_neighbors = self.data_graph[5][data_vertex]\n",
    "        q_neigh_labels = [self.pattern[1][u] for u in query_neighbors]\n",
    "        d_neigh_labels = [self.data_graph[1][v] for v in data_neighbors]\n",
    "        q_label_frequency = defaultdict(lambda:0)\n",
    "        d_label_frequency = defaultdict(lambda:0)\n",
    "        for l in q_neigh_labels:\n",
    "            q_label_frequency[l] += 1\n",
    "        for l in d_neigh_labels:\n",
    "            d_label_frequency[l] += 1\n",
    "        for l in q_neigh_labels:\n",
    "            # if the query label grequency is greater than data label frequency, return false\n",
    "            # if there is no label in data (l not in the d_dict) the number is 0 (default dict)\n",
    "            if q_label_frequency[l] > d_label_frequency[l]:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def update_query(self, pattern_info):\n",
    "        self.pattern = pattern_info\n",
    "\n",
    "    def cpp_GQL(self, query_graph_file, data_graph_file):\n",
    "        num_query_vertices = len(self.pattern[0])\n",
    "        # base_command = ['/data/hancwang/Scalable Neural Subgraph Counting/Related_work/SubgraphMatching-master/build/filter/SubgraphMatching.out', '-d', data_graph_file, '-q', query_graph_file, '-filter', 'GQL']\n",
    "        os.environ['LD_LIBRARY_PATH'] = '/home/lxhq/Documents/workspace/NeurSC/build_with_subgraph/graph:/home/lxhq/Documents/workspace/NeurSC/build_with_subgraph/utility'\n",
    "        base_command = ['build_with_subgraph/filter/SubgraphMatching.out', '-d', data_graph_file, '-q', query_graph_file, '-filter', 'GQL']\n",
    "        output = subprocess.run(base_command, capture_output=True)\n",
    "        baseline_visit = output.stdout.decode(encoding).split('\\n')\n",
    "        # print(baseline_visit)\n",
    "        candidates = list()\n",
    "        candidate_count = list()\n",
    "\n",
    "        for i in range(len(baseline_visit)):\n",
    "            if 'Candidate set is:' in baseline_visit[i]:\n",
    "                candidate_info = baseline_visit[i+1: i+2*num_query_vertices+1]\n",
    "            elif 'Candidate set version:' in baseline_visit[i]:\n",
    "                candidates = baseline_visit[i+1].split()\n",
    "                for j in range(len(candidates)):\n",
    "                    candidates[j] = int(candidates[j])\n",
    "            elif 'Subgraph List is :' in baseline_visit[i]:\n",
    "                induced_subgraph_list = baseline_visit[i+1].split()\n",
    "                for j in range(len(induced_subgraph_list)):\n",
    "                    induced_subgraph_list[j] = int(induced_subgraph_list[j])\n",
    "            elif 'Offset is :' in baseline_visit[i]:\n",
    "                neighbor_offset = baseline_visit[i+1].split()\n",
    "                for j in range(len(neighbor_offset)):\n",
    "                    neighbor_offset[j] = int(neighbor_offset[j])\n",
    "            elif 'Filter vertices' in baseline_visit[i]:\n",
    "                print(baseline_visit[i])\n",
    "        # print(what_we_need)\n",
    "        for i in range(len(candidate_info)):\n",
    "            if i%2 == 0:\n",
    "                candidate_count.append(int(candidate_info[i]))\n",
    "        \n",
    "        return candidates, candidate_count, induced_subgraph_list, neighbor_offset, candidate_info\n",
    "\n",
    "class SampleSubgraph:\n",
    "    def __init__(self, query, data_graph):\n",
    "        # data information contains:\n",
    "        # 0: id 1: label 2: degree 3: edge_info 4: edge_label 5: vertex neighbor 6: label_dict\n",
    "        self.query = query\n",
    "        self.data_graph = data_graph\n",
    "\n",
    "    def find_subgraph(self, start_query_vertex, candidates):\n",
    "        output_vertices = list()\n",
    "        output_v_label = list()\n",
    "        output_degree = list()\n",
    "        output_edges = list()\n",
    "        output_edge_label = list()\n",
    "        output_v_neigh = list()\n",
    "        depth = graph_depth(self.query, start_query_vertex)\n",
    "        candidate_u = candidates[start_query_vertex]\n",
    "        all_candidates = list()\n",
    "        for i in range(len(candidates)):\n",
    "            for j in range(len(candidates[i])):\n",
    "                all_candidates.append(candidates[i][j])\n",
    "        all_candidates = list(set(all_candidates))\n",
    "        # print(all_candidates)\n",
    "        data_label = self.data_graph[1]\n",
    "        data_neigh = self.data_graph[5]\n",
    "        # two possible ways: 1. start from all candidates and perform BFS search\n",
    "        # 2. when the candidate is visited, we don't do the search starting from that node.\n",
    "        all_need_visited = deepcopy(candidate_u)\n",
    "        while len(all_need_visited) > 0:\n",
    "            search_depth = 0\n",
    "            queue = list()\n",
    "            depth_queue = list()\n",
    "            new_graph_vertices = list()\n",
    "            new_graph_v_label = dict()\n",
    "            new_graph_v_degree = defaultdict(lambda : 0)\n",
    "            new_e_u = list()\n",
    "            new_e_v = list()\n",
    "            new_edge_label = list()\n",
    "            new_graph_v_neigh = defaultdict(list)\n",
    "            start_data_vertex = all_need_visited.pop(0)\n",
    "            queue.append(start_data_vertex)\n",
    "            depth_queue.append(search_depth)\n",
    "            new_graph_vertices.append(start_data_vertex)\n",
    "            new_graph_v_label[start_data_vertex] = data_label[start_data_vertex]\n",
    "            while len(queue)>0:\n",
    "                current_data_vertex = queue.pop(0)\n",
    "                search_depth = depth_queue.pop(0)\n",
    "                if search_depth > depth:\n",
    "                    break\n",
    "                for v in data_neigh[current_data_vertex]:\n",
    "                    if v in all_need_visited:\n",
    "                        all_need_visited.remove(v)\n",
    "                    if v not in new_graph_vertices and v in all_candidates:\n",
    "                        new_graph_vertices.append(v)\n",
    "                        new_graph_v_label[v] = data_label[v]\n",
    "                        queue.append(v)\n",
    "                        depth_queue.append(search_depth+1)\n",
    "                        for neigh_v in data_neigh[v]:\n",
    "                            if neigh_v in new_graph_vertices:\n",
    "                                # two way (undirected) edges\n",
    "                                new_e_u.append(v)\n",
    "                                new_e_v.append(neigh_v)\n",
    "                                new_e_u.append(neigh_v)\n",
    "                                new_e_v.append(v)\n",
    "                                new_graph_v_degree[v] += 1\n",
    "                                new_graph_v_degree[neigh_v] += 1\n",
    "\n",
    "                                # neighbor should be added only once.\n",
    "                                # new_graph_v_neigh[v].append(neigh_v)\n",
    "                                new_graph_v_neigh[neigh_v].append(v)\n",
    "\n",
    "                                new_edge_label.append(1)\n",
    "                                new_edge_label.append(1)\n",
    "            new_graph_edges = [new_e_u, new_e_v]\n",
    "            output_vertices.append(new_graph_vertices)\n",
    "            output_v_label.append(deepcopy(new_graph_v_label))\n",
    "            output_degree.append(deepcopy(new_graph_v_degree))\n",
    "            output_edges.append(new_graph_edges)\n",
    "            output_edge_label.append(new_edge_label)\n",
    "            output_v_neigh.append(deepcopy(new_graph_v_neigh))\n",
    "        return output_vertices, output_v_label, output_degree, output_edges, output_edge_label, output_v_neigh\n",
    "\n",
    "    def find_subgraph_induced(self, candidates):\n",
    "        t_0 = time.time()\n",
    "        all_candidates = list()\n",
    "        for i in range(len(candidates)):\n",
    "            for j in range(len(candidates[i])):\n",
    "                all_candidates.append(candidates[i][j])\n",
    "        all_candidates = list(set(all_candidates))\n",
    "        all_need_visited = deepcopy(all_candidates)\n",
    "        queue = list()\n",
    "        depth_queue = list()\n",
    "        new_graph_vertices = list()\n",
    "        new_graph_v_label = dict()\n",
    "        new_graph_v_degree = defaultdict(lambda : 0)\n",
    "        new_e_u = list()\n",
    "        new_e_v = list()\n",
    "        new_edge_label = list()\n",
    "        new_graph_v_neigh = defaultdict(list)\n",
    "\n",
    "        # get data graph information\n",
    "        data_label = self.data_graph[1]\n",
    "        data_edge = self.data_graph[3]\n",
    "        data_neigh = self.data_graph[5]\n",
    "        \n",
    "        new_graph_vertices = deepcopy(all_candidates)\n",
    "        for v in new_graph_vertices:\n",
    "            new_graph_v_label[v] = data_label[v]\n",
    "        t_1 = time.time()\n",
    "        print('sample satage 1: {}s'.format(t_1-t_0))\n",
    "        # for i in range(len(data_edge[0])):\n",
    "        #     # if two nodes are both in candidate set, the edge is included for new graph\n",
    "        #     u = data_edge[0][i]\n",
    "        #     v = data_edge[1][i]\n",
    "        #     if u in all_candidates and v in all_candidates:\n",
    "        #         new_e_u.append(u)\n",
    "        #         new_e_v.append(v)\n",
    "        #         # only add once, since the edge will appear twice.\n",
    "        #         new_graph_v_degree[u] += 1\n",
    "        #         new_graph_v_neigh[u].append(v)\n",
    "        #         new_edge_label.append(1)\n",
    "\n",
    "        for vertex in new_graph_vertices:\n",
    "            # if two nodes are both in candidate set, the edge is included for new graph\n",
    "            neigh_of_v = data_neigh[vertex]\n",
    "            for u in neigh_of_v:\n",
    "                if u in all_candidates:\n",
    "                    new_e_u.append(u)\n",
    "                    new_e_v.append(vertex)\n",
    "                    # only add once, since the edge will appear twice.\n",
    "                    new_graph_v_degree[vertex] += 1\n",
    "                    new_graph_v_neigh[vertex].append(u)\n",
    "                    new_edge_label.append(1)\n",
    "\n",
    "        t_2 = time.time()\n",
    "        print('sample stage 2: {}s'.format(t_2-t_1))\n",
    "        new_edges = [deepcopy(new_e_u), deepcopy(new_e_v)]\n",
    "        new_vertices = new_graph_vertices\n",
    "        new_v_label = new_graph_v_label\n",
    "        new_degree = deepcopy(new_graph_v_degree)\n",
    "        new_edge_label = deepcopy(new_edge_label)\n",
    "        new_v_neigh = new_graph_v_neigh\n",
    "\n",
    "        check_info = [new_vertices, new_v_label, new_degree, new_edges, new_edge_label, new_v_neigh]\n",
    "        output_vertices, output_v_label, output_degree, output_edges, output_edge_label, output_v_neigh = self._split_graph(check_info)\n",
    "        t_3 = time.time()\n",
    "        print('sample stage 3: {}s'.format(t_3-t_2))\n",
    "        # output_graph_info = [output_vertices, output_v_label, output_degree, output_edges, output_edge_label, output_v_neigh]\n",
    "\n",
    "        return output_vertices, output_v_label, output_degree, output_edges, output_edge_label, output_v_neigh\n",
    "\n",
    "    def load_induced_subgraph(self, candidates, induced_subgraph_list, neighbor_offset):\n",
    "        queue = list()\n",
    "        depth_queue = list()\n",
    "        new_graph_vertices = list()\n",
    "        new_graph_v_label = dict()\n",
    "        new_graph_v_degree = defaultdict(lambda : 0)\n",
    "        new_e_u = list()\n",
    "        new_e_v = list()\n",
    "        new_edge_label = list()\n",
    "        new_graph_v_neigh = defaultdict(list)\n",
    "\n",
    "        # get data graph information\n",
    "        data_label = self.data_graph[1]\n",
    "        data_edge = self.data_graph[3]\n",
    "        data_neigh = self.data_graph[5]\n",
    "\n",
    "        new_graph_vertices = deepcopy(candidates)\n",
    "        for v in new_graph_vertices:\n",
    "            new_graph_v_label[v] = data_label[v]\n",
    "\n",
    "        for i in range(len(candidates)):\n",
    "            vertex = candidates[i]\n",
    "            strat_index = neighbor_offset[i]\n",
    "            end_index = neighbor_offset[i+1]\n",
    "            for j in range(strat_index, end_index):\n",
    "                u = induced_subgraph_list[j]\n",
    "                new_e_u.append(u)\n",
    "                new_e_v.append(vertex)\n",
    "                # only add once, since the edge will appear twice.\n",
    "                new_graph_v_degree[vertex] += 1\n",
    "                new_graph_v_neigh[vertex].append(u)\n",
    "                new_edge_label.append(1)\n",
    "        \n",
    "        new_edges = [deepcopy(new_e_u), deepcopy(new_e_v)]\n",
    "        new_vertices = new_graph_vertices\n",
    "        new_v_label = new_graph_v_label\n",
    "        new_degree = deepcopy(new_graph_v_degree)\n",
    "        new_edge_label = deepcopy(new_edge_label)\n",
    "        new_v_neigh = new_graph_v_neigh\n",
    "\n",
    "        check_info = [new_vertices, new_v_label, new_degree, new_edges, new_edge_label, new_v_neigh]\n",
    "        output_vertices, output_v_label, output_degree, output_edges, output_edge_label, output_v_neigh = self._split_graph(check_info)\n",
    "\n",
    "        return output_vertices, output_v_label, output_degree, output_edges, output_edge_label, output_v_neigh\n",
    "\n",
    "    def _split_graph(self, graph_info):\n",
    "        vertices_id = graph_info[0]\n",
    "        vertices_label = graph_info[1]\n",
    "        vertices_neighbor = graph_info[5]\n",
    "        num_vertices = len(vertices_id)\n",
    "        to_be_visited = deepcopy(vertices_id)\n",
    "        \n",
    "\n",
    "        # initialize the output lists\n",
    "        output_vertices = list()\n",
    "        output_v_label = list()\n",
    "        output_v_degree = list()\n",
    "        output_edges = list()\n",
    "        output_e_label = list()\n",
    "        output_v_neigh = list()\n",
    "\n",
    "        while len(to_be_visited) > 0:\n",
    "            # initialize the temp containers.\n",
    "            out_temp_vertices = list()\n",
    "            out_temp_v_label = dict()\n",
    "            out_temp_v_degree = defaultdict(lambda: 0)\n",
    "            out_temp_e_u = list()\n",
    "            out_temp_e_v = list()\n",
    "            out_temp_e_label = list()\n",
    "            out_temp_v_neigh = defaultdict(list)\n",
    "\n",
    "            start_node = to_be_visited[0]\n",
    "            # to_be_visited.remove(start_node)\n",
    "            queue = list()\n",
    "            queue.append(start_node)\n",
    "\n",
    "            while len(queue) > 0:\n",
    "                current_node = queue.pop(0)\n",
    "                current_neighbors = vertices_neighbor[current_node]\n",
    "                try:\n",
    "                    to_be_visited.remove(current_node)\n",
    "                except ValueError:\n",
    "                    # print('node {} has been removed'.format(current_node))\n",
    "                    continue   # if there is no this node in the to be visited set, we donot need to compute it again. will lead to bugs.  \n",
    "                out_temp_vertices.append(current_node)        \n",
    "                out_temp_v_label[current_node] = vertices_label[current_node]         \n",
    "                for v in current_neighbors:\n",
    "                    # a BFS, do we need to check whether it is in the to be visited set? (in Queue it should!)\n",
    "                    out_temp_e_u.append(current_node)\n",
    "                    out_temp_e_v.append(v)       # add a one-way edge, it will be added again.\n",
    "                    out_temp_e_label.append(1)   # edge label is always 1.\n",
    "                    out_temp_v_degree[current_node] += 1\n",
    "                    out_temp_v_neigh[current_node].append(v)\n",
    "                    if v in to_be_visited:\n",
    "                        queue.append(v)\n",
    "\n",
    "            output_vertices.append(deepcopy(out_temp_vertices))\n",
    "            output_v_label.append(deepcopy(out_temp_v_label))\n",
    "            output_v_degree.append(deepcopy(out_temp_v_degree))\n",
    "            output_edges.append([deepcopy(out_temp_e_u), deepcopy(out_temp_e_v)])\n",
    "            output_e_label.append(deepcopy(out_temp_e_label))\n",
    "            output_v_neigh.append(deepcopy(out_temp_v_neigh))\n",
    "        \n",
    "        return output_vertices, output_v_label, output_v_degree, output_edges, output_e_label, output_v_neigh\n",
    "\n",
    "    def update_query(self, query):\n",
    "        self.query = query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_graph_path = '../dataset/yeast/data_graph/yeast.graph'\n",
    "    query_graph_path = '../dataset/yeast/query_graph/query_dense_4_1.graph'\n",
    "    data_graph = load_graph(data_graph_path)\n",
    "    query_graph = load_graph(query_graph_path)\n",
    "    filter = Filtering(query_graph, data_graph)\n",
    "    subgraph_sampler = SampleSubgraph(query_graph, data_graph)\n",
    "    candidates, candidate_count, induced_subgraph_list, neighbor_offset, candidate_info = filter.cpp_GQL(query_graph_path, data_graph_path)\n",
    "    starting_vertex = candidate_count.index(min(candidate_count))\n",
    "    starting_vertex = 0\n",
    "    print('candidates', candidates)\n",
    "    print('candidate_count', candidate_count)\n",
    "    print('induced_subgraph_list', induced_subgraph_list)\n",
    "    print('neighbor_offset', neighbor_offset)\n",
    "    print('candidate_info', candidate_info)\n",
    "    print('starting_vertex', starting_vertex)\n",
    "    \n",
    "    vertices_candidates = list()\n",
    "    for i in range(len(candidate_info)):\n",
    "        if i % 2 == 1:\n",
    "            vertices_candidates.append(list(map(int, candidate_info[i].split())))\n",
    "    new_vertices, new_v_label, new_degree, new_edges, new_e_label, new_v_neigh = subgraph_sampler.find_subgraph(starting_vertex, candidates=vertices_candidates)\n",
    "    print(new_vertices)\n",
    "    print(new_v_label)\n",
    "    print(new_degree)\n",
    "    print(new_edges)\n",
    "    print(new_e_label)\n",
    "    print(new_v_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove query files without a true count\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def load_true_card(true_card_path):\n",
    "    res = set()\n",
    "    with open(true_card_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            tokens = line.split(',')\n",
    "            name = tokens[0]\n",
    "            res.add(name)\n",
    "    return res\n",
    "\n",
    "true_path = 'data/yeast/query_graph.csv'\n",
    "true_cards = load_true_card(true_path)\n",
    "\n",
    "for file in os.listdir('data/yeast/query_graph/'):\n",
    "    name = file.split('.')[0]\n",
    "    if name not in true_cards:\n",
    "        os.remove('data/yeast/query_graph/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the outputs env\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "shutil.rmtree('saved_models', ignore_errors=True)\n",
    "shutil.rmtree('saved_params', ignore_errors=True)\n",
    "shutil.rmtree('saved_results', ignore_errors=True)\n",
    "\n",
    "os.mkdir('saved_models/')\n",
    "os.mkdir('saved_params/')\n",
    "os.mkdir('saved_results/')\n",
    "\n",
    "shutil.rmtree('outputs', ignore_errors=True)\n",
    "os.mkdir('outputs/')\n",
    "os.mkdir('outputs/yeast/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw figures for NeurSC results\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_result_file(result_dir, params_dir, result_name):\n",
    "    # model_data = [['Epochs', 'Train Query Number', 'Train Query Number Type', 'Query Size', 'Query Type', 'Pred', 'Card', 'q-error'], [...], ...]\n",
    "    name_tokens = result_name.split('_')\n",
    "    epochs = int(name_tokens[4])\n",
    "    training_type = None\n",
    "    training_prec = None\n",
    "    train_query_suffix = ''\n",
    "    if name_tokens[7] == 'aug':\n",
    "        training_prec = name_tokens[9]\n",
    "        if name_tokens[8] == '1':\n",
    "            training_type = 'aug_1'\n",
    "            train_query_suffix = 'All original\\nqueries +\\n{}% aug 1\\nqueries'.format(training_prec)\n",
    "        else:\n",
    "            training_type = 'aug_2'\n",
    "            train_query_suffix = 'All original\\nqueries +\\nall aug 1\\nqueries +\\n{}% aug 2\\nqueries'.format(training_prec)\n",
    "    else:\n",
    "        training_type = 'original'\n",
    "        training_prec = name_tokens[7]\n",
    "        train_query_suffix = '{}% original\\nqueries'.format(training_prec)\n",
    "    res = []\n",
    "\n",
    "    train_query_number = 0\n",
    "    with open(params_dir + result_name, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if line.startswith('training query number'):\n",
    "                train_query_number = int(line.split(':')[1].strip())\n",
    "                train_query_number = str(train_query_number) + '\\n' + train_query_suffix\n",
    "                break\n",
    "\n",
    "    with open(result_dir + result_name, 'r') as f:\n",
    "        for line in f.readlines()[1:]:\n",
    "            line_tokens = line.split()\n",
    "            query_name_tokens = line_tokens[0].split('.')[0].split('_')\n",
    "            query_size = int(query_name_tokens[2])\n",
    "            query_type = query_name_tokens[1]\n",
    "            pred = float(line_tokens[2])\n",
    "            card = float(line_tokens[3])\n",
    "            if pred < card:\n",
    "                q_error = -math.log10(float(line_tokens[1]))\n",
    "            else:\n",
    "                q_error = math.log10(float(line_tokens[1]))\n",
    "            res.append([epochs, train_query_number, training_type + '_' + training_prec, query_size, query_type, pred, card, q_error])\n",
    "    return res\n",
    "\n",
    "def draw_box_plot(dataframe, title, fontsize=16):\n",
    "    sns.set(rc={'figure.figsize':(20.7,8.27)})\n",
    "    bp = sns.boxplot(data=df, x='Train Query Number', y='q-error', whis=[1, 99])\n",
    "    vertical_lines = [8.5, 11.5]  # List of x-axis values where you want to add vertical line\n",
    "    for line in vertical_lines:\n",
    "        plt.axvline(x=line, color='red', linestyle='--')\n",
    "    plt.grid(visible=True, linestyle='--')\n",
    "    plt.title(title, fontsize=fontsize)\n",
    "    plt.axhline(0, color='green',linestyle='dashed')\n",
    "    plt.ylabel('Under estimate <--- q-error ---> Over estimate', fontsize=fontsize)\n",
    "    plt.xlabel('Number of training queries', fontsize=fontsize)\n",
    "    if title == 'yeast':\n",
    "        plt.yticks(ticks=[-6, -4, -2, 0, 2, 4, 6], \n",
    "                   labels=['$10^6$', '$10^4$', '$10^2$', '$10^0$', '$10^2$', '$10^4$', '$10^6$'])\n",
    "    elif title == 'youtube':    \n",
    "        plt.yticks(ticks=[-8, -6, -4, -2, 0, 2, 4, 6, 8], \n",
    "                   labels=['$10^8$', '$10^6$', '$10^4$', '$10^2$', '$10^0$', '$10^2$', '$10^4$', '$10^6$', '$10^8$'])\n",
    "    elif title == 'wordnet':\n",
    "        plt.yticks(ticks=[-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5], \n",
    "                   labels=['$10^5$', '$10^4$', '$10^3$', '$10^2$', '$10^1$', '$10^0$', '$10^1$', '$10^2$', '$10^3$', '$10^4$', '$10^5$'])\n",
    "    elif title == 'eu2005':\n",
    "        plt.yticks(ticks=[-3, -2, -1, 0, 1, 2, 3], \n",
    "                   labels=['$10^3$', '$10^2$', '$10^1$', '$10^0$', '$10^1$', '$10^2$', '$10^3$'])\n",
    "    else:\n",
    "        raise Exception('Not recognized dataset')\n",
    "    plt.ylabel('under estimate <--- q-error ---> over estimate')\n",
    "    plt.xlabel('number of training queries')\n",
    "    pcntls = df.groupby('Train Query Number')['q-error'].describe(percentiles=[0.1, 0.9])\n",
    "    pcntls = pcntls.sort_values(by='Train Query Number', key=lambda x: x.apply(lambda y: int(y.split('\\n')[0])))\n",
    "    display(pcntls)\n",
    "    columns = len(pcntls['10%'])\n",
    "    bp.scatter(data=pcntls, x=range(columns), y='10%', marker='x')\n",
    "    bp.scatter(data=pcntls, x=range(columns), y='90%', marker='x')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_line_plot(df, title, fontsize=16):\n",
    "    medians = df.groupby(by=['Train Query Number', 'Epochs'])['q-error'].median().reset_index(name='median q-error')\n",
    "    medians = medians.sort_values(by='Train Query Number', key=lambda x: x.apply(lambda y: int(y.split('\\n')[0])))\n",
    "    medians['median q-error'] = medians['median q-error'].abs()\n",
    "    sns.lineplot(data=medians, x='Train Query Number', y='median q-error')\n",
    "    plt.ylabel('log10(median q-error)', fontsize=fontsize)\n",
    "    plt.title(title, fontsize=fontsize)\n",
    "    plt.xlabel('Number of training queries', fontsize=fontsize)\n",
    "    plt.tick_params(axis='y', labelsize=fontsize)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_graph = 'yeast'\n",
    "    result_dir = 'saved_results/'\n",
    "    params_dir = 'saved_params/'\n",
    "    # model_data = [['Epochs', 'Train Query Number', 'Train Query Number Type', 'Query Size', 'Query Type', 'Pred', 'Card', 'q-error'], [...], ...]\n",
    "    model_data = []\n",
    "    for file in os.listdir(result_dir):\n",
    "        if file.startswith(data_graph):\n",
    "            model_data.extend(read_result_file(result_dir, params_dir, file))\n",
    "    df = pd.DataFrame(data=model_data, \n",
    "                      columns=['Epochs', 'Train Query Number', 'Train Query Number Type', 'Query Size', ' QueryType', 'Pred', 'Card', 'q-error'])\n",
    "    df = df.loc[df['Epochs'] == 80]\n",
    "    df = df.sort_values(by='Train Query Number', key=lambda x: x.apply(lambda y: int(y.split('\\n')[0])))\n",
    "    draw_box_plot(df, data_graph)\n",
    "    draw_line_plot(df, data_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw figures for NeurSC results\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_result_file(result_dir, params_dir, result_name):\n",
    "    # model_data = [['Epochs', 'Train Query Number', 'Train Query Number Type', 'Query Size', 'Query Type', 'Pred', 'Card', 'q-error'], [...], ...]\n",
    "    name_tokens = result_name.split('_')\n",
    "    epochs = int(name_tokens[4])\n",
    "    training_type = None\n",
    "    training_prec = None\n",
    "    train_query_suffix = ''\n",
    "    if name_tokens[7] == 'aug':\n",
    "        training_prec = name_tokens[9]\n",
    "        if name_tokens[8] == '1':\n",
    "            training_type = 'aug_1'\n",
    "            train_query_suffix = 'All original\\nqueries +\\n{}% aug 1\\nqueries'.format(training_prec)\n",
    "        else:\n",
    "            training_type = 'aug_2'\n",
    "            train_query_suffix = 'All original\\nqueries +\\nall aug 1\\nqueries +\\n{}% aug 2\\nqueries'.format(training_prec)\n",
    "    else:\n",
    "        training_type = 'original'\n",
    "        training_prec = name_tokens[7]\n",
    "        train_query_suffix = '{}% original\\nqueries'.format(training_prec)\n",
    "    res = []\n",
    "\n",
    "    train_query_number = 0\n",
    "    with open(params_dir + result_name, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if line.startswith('training query number'):\n",
    "                train_query_number = int(line.split(':')[1].strip())\n",
    "                train_query_number = str(train_query_number) + '\\n' + train_query_suffix\n",
    "                break\n",
    "\n",
    "    with open(result_dir + result_name, 'r') as f:\n",
    "        for line in f.readlines()[1:]:\n",
    "            line_tokens = line.split()\n",
    "            query_name_tokens = line_tokens[0].split('.')[0].split('_')\n",
    "            query_size = int(query_name_tokens[2])\n",
    "            query_type = query_name_tokens[1]\n",
    "            pred = float(line_tokens[2])\n",
    "            card = float(line_tokens[3])\n",
    "            if pred < card:\n",
    "                q_error = -math.log10(float(line_tokens[1]))\n",
    "            else:\n",
    "                q_error = math.log10(float(line_tokens[1]))\n",
    "            res.append([epochs, train_query_number, training_type + '_' + training_prec, query_size, query_type, pred, card, q_error])\n",
    "    return res\n",
    "\n",
    "def draw_box_plot(dataframe, title, fontsize=16):\n",
    "    sns.set(rc={'figure.figsize':(20.7,8.27)})\n",
    "    bp = sns.boxplot(data=df, x='Train Query Number', y='q-error', whis=[1, 99])\n",
    "    vertical_lines = [8.5, 11.5]  # List of x-axis values where you want to add vertical line\n",
    "    for line in vertical_lines:\n",
    "        plt.axvline(x=line, color='red', linestyle='--')\n",
    "    plt.grid(visible=True, linestyle='--')\n",
    "    plt.title(title, fontsize=fontsize)\n",
    "    plt.axhline(0, color='green',linestyle='dashed')\n",
    "    plt.ylabel('Under estimate <--- q-error ---> Over estimate', fontsize=fontsize)\n",
    "    plt.xlabel('Number of training queries', fontsize=fontsize)\n",
    "    if title == 'yeast':\n",
    "        plt.yticks(ticks=[-6, -4, -2, 0, 2, 4, 6], \n",
    "                   labels=['$10^6$', '$10^4$', '$10^2$', '$10^0$', '$10^2$', '$10^4$', '$10^6$'])\n",
    "    elif title == 'youtube':    \n",
    "        plt.yticks(ticks=[-8, -6, -4, -2, 0, 2, 4, 6, 8], \n",
    "                   labels=['$10^8$', '$10^6$', '$10^4$', '$10^2$', '$10^0$', '$10^2$', '$10^4$', '$10^6$', '$10^8$'])\n",
    "    elif title == 'wordnet':\n",
    "        plt.yticks(ticks=[-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5], \n",
    "                   labels=['$10^5$', '$10^4$', '$10^3$', '$10^2$', '$10^1$', '$10^0$', '$10^1$', '$10^2$', '$10^3$', '$10^4$', '$10^5$'])\n",
    "    elif title == 'eu2005':\n",
    "        plt.yticks(ticks=[-3, -2, -1, 0, 1, 2, 3], \n",
    "                   labels=['$10^3$', '$10^2$', '$10^1$', '$10^0$', '$10^1$', '$10^2$', '$10^3$'])\n",
    "    else:\n",
    "        raise Exception('Not recognized dataset')\n",
    "    plt.ylabel('under estimate <--- q-error ---> over estimate')\n",
    "    plt.xlabel('number of training queries')\n",
    "    pcntls = df.groupby('Train Query Number')['q-error'].describe(percentiles=[0.1, 0.9])\n",
    "    pcntls = pcntls.sort_values(by='Train Query Number', key=lambda x: x.apply(lambda y: int(y.split('\\n')[0])))\n",
    "    display(pcntls)\n",
    "    columns = len(pcntls['10%'])\n",
    "    bp.scatter(data=pcntls, x=range(columns), y='10%', marker='x')\n",
    "    bp.scatter(data=pcntls, x=range(columns), y='90%', marker='x')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_line_plot(df, title, fontsize=16):\n",
    "    medians = df.groupby(by=['Train Query Number', 'Epochs'])['q-error'].median().reset_index(name='median q-error')\n",
    "    medians = medians.sort_values(by='Train Query Number', key=lambda x: x.apply(lambda y: int(y.split('\\n')[0])))\n",
    "    medians['median q-error'] = medians['median q-error'].abs()\n",
    "    sns.lineplot(data=medians, x='Train Query Number', y='median q-error')\n",
    "    plt.ylabel('log10(median q-error)', fontsize=fontsize)\n",
    "    plt.title(title, fontsize=fontsize)\n",
    "    plt.xlabel('Number of training queries', fontsize=fontsize)\n",
    "    plt.tick_params(axis='y', labelsize=fontsize)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_graph = 'yeast'\n",
    "    result_dir = 'saved_results/'\n",
    "    params_dir = 'saved_params/'\n",
    "    # model_data = [['Epochs', 'Train Query Number', 'Train Query Number Type', 'Query Size', 'Query Type', 'Pred', 'Card', 'q-error'], [...], ...]\n",
    "    model_data = []\n",
    "    for file in os.listdir(result_dir):\n",
    "        if file.startswith(data_graph):\n",
    "            model_data.extend(read_result_file(result_dir, params_dir, file))\n",
    "    df = pd.DataFrame(data=model_data, \n",
    "                      columns=['Epochs', 'Train Query Number', 'Train Query Number Type', 'Query Size', ' QueryType', 'Pred', 'Card', 'q-error'])\n",
    "    df = df.loc[df['Epochs'] == 80]\n",
    "    df = df.sort_values(by='Train Query Number', key=lambda x: x.apply(lambda y: int(y.split('\\n')[0])))\n",
    "    draw_box_plot(df, data_graph)\n",
    "    draw_line_plot(df, data_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_set = 'eu2005'\n",
    "    output_file = open('outputs/research/{}.csv'.format(data_set), 'w')\n",
    "    data_graph_path = '../dataset/{}/data_graph/{}.graph'.format(data_set, data_set)\n",
    "    query_graph_dir = '../dataset/{}/query_graph/'.format(data_set)\n",
    "    data_graph = load_graph(data_graph_path)\n",
    "    for file in os.listdir(query_graph_dir):\n",
    "        query_size = int(file.split('_')[2])\n",
    "        query_graph_path = query_graph_dir + file\n",
    "        query_graph = load_graph(query_graph_path)\n",
    "        filter = Filtering(query_graph, data_graph)\n",
    "        subgraph_sampler = SampleSubgraph(query_graph, data_graph)\n",
    "        candidates, candidate_count, induced_subgraph_list, neighbor_offset, candidate_info = filter.cpp_GQL(query_graph_path, data_graph_path)\n",
    "        starting_vertex = candidate_count.index(min(candidate_count))\n",
    "        starting_vertex = 0\n",
    "        output_file.write(str(query_size) + ',' + str(len(candidates)) + ',' + str(len(data_graph[0])) + ',' + file + '\\n')\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpreting the test results\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def get_prediction_statistics(errors: list):\n",
    "\tlower, upper = np.quantile(errors, 0.25), np.quantile(errors, 0.75)\n",
    "\tprint(\"<\" * 80, flush=True)\n",
    "\tprint(\"Predict Result Profile of {} Queries:\".format(len(errors)), flush=True)\n",
    "\tprint(\"Min/Max: {:.4f} / {:.4f}\".format(np.min(errors), np.max(errors)), flush=True)\n",
    "\tprint(\"Mean: {:.4f}\".format(np.mean(errors)), flush=True)\n",
    "\tprint(\"Median: {:.4f}\".format(np.median(errors)), flush=True)\n",
    "\tprint(\"25%/75% Quantiles: {:.4f} / {:.4f}\".format(lower, upper), flush=True)\n",
    "\tprint(\">\" * 80, flush=True)\n",
    "\terror_median = abs(upper - lower)\n",
    "\treturn error_median\n",
    "\n",
    "results_dir = './saved_results/'\n",
    "all_results = []\n",
    "for file in os.listdir(results_dir):\n",
    "    results = {} # results <- {name: [pred, true]}\n",
    "    with open(results_dir + file) as f:\n",
    "        for line in f.readlines():\n",
    "            if line.startswith('f'):\n",
    "                continue\n",
    "            line = line.strip()\n",
    "            tokens = line.split(' ')\n",
    "            results[tokens[0]] = [float(tokens[2]), float(tokens[3])]\n",
    "    all_results.append(results)\n",
    "\n",
    "for results in all_results:\n",
    "    log2_q_errors = []\n",
    "    total_log2_mse_loss = 0\n",
    "    total_log2_l1_loss = 0\n",
    "    count = 0\n",
    "    for pred, card in results.values():\n",
    "        if pred == 0:\n",
    "             pred = 1\n",
    "        pred_log2 = math.log2(pred)\n",
    "        card_log2 = math.log2(card)\n",
    "        log2_q_errors.append(pred_log2 - card_log2)\n",
    "        total_log2_mse_loss += (pred_log2 - card_log2)**2\n",
    "        total_log2_l1_loss += abs(pred_log2 - card_log2)\n",
    "        count += 1   \n",
    "    print(\"Evaluation result of Eval dataset: Total Loss= {:.4f}, Total L1 Loss= {:.4f}\".format(total_log2_mse_loss, total_log2_l1_loss))\n",
    "    get_prediction_statistics(log2_q_errors)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
